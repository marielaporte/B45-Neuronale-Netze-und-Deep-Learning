{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRq6oqhts02u",
        "outputId": "ce020aa5-08b2-4198-913e-117c6a67d04d"
      },
      "id": "qRq6oqhts02u",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3bf7453",
      "metadata": {
        "id": "f3bf7453"
      },
      "source": [
        "Wählen Sie zwischen den beiden Hausaufgaben eine aus und implementieren Sie diese, gern auch als Kleingruppe."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34359292",
      "metadata": {
        "id": "34359292"
      },
      "source": [
        "## Aufgabe 1\n",
        "\n",
        "\n",
        "\n",
        "Implementieren Sie ein Verfahren zur Berechnung von Ähnlichkeiten zwischen Wörtern basierend auf der Distributional Hypothesis, indem Sie für jedes Wort im Vokabular zählen, wie häufig die anderen Wörter in einer  Umgebung fixer Länge vorkommen (siehe Vorlesungsmaterial). Sie können eines der folgenden Korpora oder einen eigens gewählten Korpus verwenden:\n",
        "\n",
        "1. Kochrezepte (`recipes.json`)\n",
        "2. Movie-Reviews (`movies.csv`)\n",
        "\n",
        "Führen Sie hierfür die aus Ihrer Sicht **notwendigen** Schritte durch, mindestens jedoch:\n",
        "\n",
        "- a) Tokenisierung\n",
        "- b) Ggf. Wort-Normalisierung (z.B. Lowercasing, Lemmatisierung, usw.) \n",
        "- c) Term-Term-Matrix (basierend auf einfacher Zählung von Vorkommnissen). Hier wählen Sie selbst, was Sie als **Umgebung/Kontext eines Wortes** betrachten. Es kann das ganze Dokument sein, der Satz, in dem das Wort vorkommt, oder 2 (3 oder 4) Wörter davor und danach. Sie können auch mehrere Varianten ausprobieren.\n",
        "- e) Wählen Sie anschließend **5 Terme** aus und und ermitteln jeweils die **10 ähnlichsten Terme** basierend auf der Term-Term-Matrix und der Kosinus-Distanz. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/SoSe2023/B45 Neuronale Netze und Deep Learning/week_6/movies.csv', header=None, names=['review', 'sentiment'])"
      ],
      "metadata": {
        "id": "pm_rivUmst3R"
      },
      "id": "pm_rivUmst3R",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisierung des Textes in einzelne Wörter\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df['tokens'] = df['review'].apply(lambda x: word_tokenize(x.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEFHGxKltbl9",
        "outputId": "9adc0963-770f-4874-bf07-187c73a5fa36"
      },
      "id": "dEFHGxKltbl9",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisierung der Wörter durch Lemmatisierung\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VNHgcC5tRg7",
        "outputId": "f44d1f2e-23a6-4c41-890c-d1d5d765d6e9"
      },
      "id": "_VNHgcC5tRg7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellung einer Term-Term-Matrix auf der Grundlage einer Umgebung mit fester Länge. \n",
        "# Hier verwenden wir ein Fenster von 5 Wörtern (2 Wörter vor und nach jedem Zielwort)\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def create_term_term_matrix(docs, window_size=5):\n",
        "    vocab = set()\n",
        "    cooccur = defaultdict(lambda: defaultdict(int))\n",
        "    for doc in docs:\n",
        "        for i, word in enumerate(doc):\n",
        "            vocab.add(word)\n",
        "            for j in range(max(0, i-window_size), min(i+window_size+1, len(doc))):\n",
        "                if i != j:\n",
        "                    cooccur[word][doc[j]] += 1\n",
        "\n",
        "    vocab = sorted(vocab)\n",
        "    term_term_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "    for i, term_i in enumerate(vocab):\n",
        "        for j, term_j in enumerate(vocab):\n",
        "            if i == j:\n",
        "                term_term_matrix[i, j] = 1.0\n",
        "            else:\n",
        "                term_term_matrix[i, j] = cooccur[term_i][term_j] + cooccur[term_j][term_i]\n",
        "\n",
        "    return term_term_matrix, vocab"
      ],
      "metadata": {
        "id": "YmcacIx8t37g"
      },
      "id": "YmcacIx8t37g",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellung einer Term-Term-Matrix auf der Grundlage der tokenisierten Bewertungen\n",
        "term_term_matrix, vocab = create_term_term_matrix(df['tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "W4Re9o2uuBHh",
        "outputId": "d83250ca-ee48-47c4-c2db-12fd59c42d7c"
      },
      "id": "W4Re9o2uuBHh",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-de0ebdfb232e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Erstellung einer Term-Term-Matrix auf der Grundlage der tokenisierten Bewertungen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mterm_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_term_term_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-b3feb42438d5>\u001b[0m in \u001b[0;36mcreate_term_term_matrix\u001b[0;34m(docs, window_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Erstellung einer Term-Term-Matrix auf der Grundlage einer Umgebung mit fester Länge.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Hier verwenden wir ein Fenster von 5 Wörtern (2 Wörter vor und nach jedem Zielwort)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Berechnung der Kosinusähnlichkeit zwischen jedem Begriffspaar im Vokabular anhand der Term-Term-Matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(term_term_matrix)"
      ],
      "metadata": {
        "id": "Beap-BDHuSZk"
      },
      "id": "Beap-BDHuSZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auswahl von 5 Zielbegriffen und Ermittlung der 10 ähnlichsten Begriffe für jeden auf der Grundlage der Kosinus-Ähnlichkeitsmatrix\n",
        "\n",
        "target_terms = ['good', 'bad', 'great', 'awful', 'love']\n",
        "num_similar_terms = 10\n",
        "\n",
        "for term in target_terms:\n",
        "    term_index = vocab.index(term)\n",
        "    similar_terms = [(vocab[i], similarity_matrix[term_index, i]) for i in range(len(vocab)) if vocab[i] != term]\n",
        "    similar_terms = sorted(similar_terms, key=lambda x: x[1], reverse=True)[:num_similar_terms]\n",
        "    print(f\"Similar terms for '{term}':\")\n",
        "    for similar_term in similar_terms:\n",
        "        print(f\"{similar_term[0]} ({similar_term[1]:.2f})\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "rTPV9rbJueSP"
      },
      "id": "rTPV9rbJueSP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9d0a0b50",
      "metadata": {
        "id": "9d0a0b50"
      },
      "source": [
        "## Aufgabe 2\n",
        "\n",
        "Implementieren Sie Shannon's Methode basierend auf Bigrammen aus der Vorlesung anhand eines frei wählbaren Korpus. Wenn möglich schreiben Sie Ihren Code so, dass man leicht auch Trigramme oder beliebige N-Gramme statt Bigrammen nutzen kann. Implementieren Sie ebenfalls eine Funktion, mit welcher Text generiert werden kann. Der Text soll so lange generiert werden, bis das Satz-Ende Symbol erzeugt wird, oder bis eine vorgegebene textlänge erreicht wird.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of Shannon's method based on bigrams using NLTK and the movie reviews dataset"
      ],
      "metadata": {
        "id": "svCpkTmXwWhf"
      },
      "id": "svCpkTmXwWhf"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "F6C6XH_AxRvq"
      },
      "id": "F6C6XH_AxRvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the movie reviews dataset\n",
        "with open('/content/drive/MyDrive/SoSe2023/B45 Neuronale Netze und Deep Learning/week_6/movies.csv', 'r', encoding='utf-8') as f:\n",
        "    reviews = f.readlines()\n",
        "\n",
        "# Tokenize the reviews into individual words\n",
        "reviews = [word_tokenize(review.strip().lower()) for review in reviews]"
      ],
      "metadata": {
        "id": "gNHbUlfWxZb-"
      },
      "id": "gNHbUlfWxZb-",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4bd9e86d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd9e86d",
        "outputId": "4d8986ff-1be8-49f7-994f-6497e8d7499f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i actually happened , so karla , believe that did to suck .\n"
          ]
        }
      ],
      "source": [
        "# Create a frequency distribution of bigrams\n",
        "bigrams = defaultdict(lambda: defaultdict(int))\n",
        "for review in reviews:\n",
        "    for i in range(len(review)-1):\n",
        "        bigrams[review[i]][review[i+1]] += 1\n",
        "\n",
        "# Compute the conditional probabilities of each word given its predecessor\n",
        "for word in bigrams:\n",
        "    total_count = sum(bigrams[word].values())\n",
        "    for next_word in bigrams[word]:\n",
        "        bigrams[word][next_word] /= total_count\n",
        "\n",
        "# Define a function to generate text based on the bigram model\n",
        "def generate_text(start_word, max_length=None):\n",
        "    text = [start_word]\n",
        "    current_word = start_word\n",
        "    length = 0\n",
        "    while True:\n",
        "        next_word = random.choices(list(bigrams[current_word].keys()), list(bigrams[current_word].values()))[0]\n",
        "        text.append(next_word)\n",
        "        current_word = next_word\n",
        "        length += 1\n",
        "        if next_word == '.' or (max_length is not None and length >= max_length):\n",
        "            break\n",
        "    return ' '.join(text)\n",
        "\n",
        "# Generate a sample sentence starting with the word \"the\"\n",
        "print(generate_text('the'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trigrams or arbitrary N-grams"
      ],
      "metadata": {
        "id": "gNanMhkvwaiw"
      },
      "id": "gNanMhkvwaiw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a frequency distribution of trigrams\n",
        "trigrams = defaultdict(lambda: defaultdict(int))\n",
        "for review in reviews:\n",
        "    for i in range(len(review)-2):\n",
        "        trigrams[(review[i], review[i+1])][review[i+2]] += 1\n",
        "\n",
        "# Compute the conditional probabilities of each word given its two predecessors\n",
        "for word_pair in trigrams:\n",
        "    total_count = sum(trigrams[word_pair].values())\n",
        "    for next_word in trigrams[word_pair]:\n",
        "        trigrams[word_pair][next_word] /= total_count\n",
        "\n",
        "# Define a function to generate text based on the trigram model\n",
        "def generate_text(start_word_pair, max_length=None):\n",
        "    text = list(start_word_pair)\n",
        "    current_word_pair = start_word_pair\n",
        "    length = 0\n",
        "    while True:\n",
        "        next_word = random.choices(list(trigrams[current_word_pair].keys()), list(trigrams[current_word_pair].values()))[0]\n",
        "        text.append(next_word)\n",
        "        current_word_pair = tuple(text[-2:])\n",
        "        length += 1\n",
        "        if next_word == '.' or (max_length is not None and length >= max_length):\n",
        "            break\n",
        "    return ' '.join(text)\n",
        "\n",
        "# Generate a sample sentence starting with the words \"the movie\"\n",
        "print(generate_text(('the', 'movie')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVIFOJzBwka8",
        "outputId": "9b7e601b-b958-4341-c738-72b2e45f2308"
      },
      "id": "SVIFOJzBwka8",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie is a precious performance by alyssa milano is something i 've seen a movie so much more depressing things happen .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a frequency distribution of 4-grams\n",
        "fivegrams = defaultdict(lambda: defaultdict(int))\n",
        "for review in reviews:\n",
        "    for i in range(len(review)-3):\n",
        "        fivegrams[(review[i], review[i+1], review[i+2])][review[i+3]] += 1\n",
        "\n",
        "# Compute the conditional probabilities of each word given its four predecessors\n",
        "for word_tuple in fivegrams:\n",
        "    total_count = sum(fivegrams[word_tuple].values())\n",
        "    for next_word in fivegrams[word_tuple]:\n",
        "        fivegrams[word_tuple][next_word] /= total_count\n",
        "\n",
        "# Define a function to generate text based on the 4-gram model\n",
        "def generate_text(start_word_tuple, max_length=None):\n",
        "    text = list(start_word_tuple)\n",
        "    current_word_tuple = start_word_tuple\n",
        "    length = 0\n",
        "    while True:\n",
        "        if current_word_tuple not in fivegrams:\n",
        "            current_word_tuple = random.choice(list(fivegrams.keys()))\n",
        "            text = list(current_word_tuple)\n",
        "        next_word = random.choices(list(fivegrams[current_word_tuple].keys()), list(fivegrams[current_word_tuple].values()))[0]\n",
        "        text.append(next_word)\n",
        "        current_word_tuple = tuple(text[-3:])\n",
        "        length += 1\n",
        "        if next_word == '.' or (max_length is not None and length >= max_length):\n",
        "            break\n",
        "    return ' '.join(text)\n",
        "\n",
        "# Generate a sample sentence starting with the words \"the movie was really\"\n",
        "print(generate_text(('the', 'movie', 'was')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80NebRnxx84h",
        "outputId": "c5c6c7e7-9129-42df-a3f4-5be4539626d9"
      },
      "id": "80NebRnxx84h",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was originally conceived as part of the film : some violent confrontations involving the angels , also gives us a great love story told with the disaster as a backdrop and a portrayal of one of the best of 1998 .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}